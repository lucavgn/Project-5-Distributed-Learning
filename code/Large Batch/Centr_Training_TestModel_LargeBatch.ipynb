{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Centr_Training_TestModel_LargeBatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtOx0ESVnO0u",
        "outputId": "0cefc4b2-91f5-4ba3-c1fe-a8b9b4b0ff86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting large_batch_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile large_batch_training.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split\n",
        "from google.colab import files\n",
        "#Compute the mean and std of CIFAR-100 dataset.\n",
        "def compute_mean_std(dataset):\n",
        "\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model\n",
        "net = LeNet5().to(device)\n",
        "\n",
        "# Optimizer Subclasses\n",
        "class LARS(optim.Optimizer):\n",
        "    def __init__(self, params, lr, momentum=0.9, weight_decay=0, trust_coefficient=0.001):\n",
        "        # Initialize the optimizer with the learning rate, momentum, weight decay, and trust coefficient\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coefficient=trust_coefficient)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            # If a closure is provided, evaluate it (commonly used for re-evaluating loss)\n",
        "            loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                   # Skip parameters that have no gradient\n",
        "                    continue\n",
        "                grad = p.grad.data # Get the gradient of the parameter\n",
        "                # Retrieve or initialize the optimizer's internal state\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['momentum_buffer'] = torch.clone(grad).detach()\n",
        "                # Apply weight decay directly to the gradient\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad.add_(p.data, alpha=group['weight_decay'])\n",
        "                # Compute the norms of the parameter and its gradient\n",
        "                param_norm = torch.norm(p.data)\n",
        "                grad_norm = torch.norm(grad)\n",
        "                # Compute local learning rate based on the trust coefficient and norms\n",
        "                if param_norm > 0 and grad_norm > 0:\n",
        "                    local_lr = group['trust_coefficient'] * param_norm / (grad_norm + 1e-8)\n",
        "                    grad = grad.mul(local_lr) # Scale the gradient with the local learning rate\n",
        "                # Update the momentum buffer\n",
        "                momentum_buffer = state['momentum_buffer']\n",
        "                momentum_buffer.mul_(group['momentum']).add_(grad) #Momentum update\n",
        "                state['momentum_buffer'] = momentum_buffer\n",
        "                # Update the parameter\n",
        "                p.data.add_(momentum_buffer, alpha=-group['lr']) # Gradient descent step with learning rate\n",
        "        return loss\n",
        "\n",
        "class LAMB(optim.Optimizer):\n",
        "    def __init__(self, params, lr, weight_decay=0, betas=(0.9, 0.999), eps=1e-8):\n",
        "        # Initialize the optimizer with learning rate, weight decay, betas for moment updates, and epsilon for stability\n",
        "        defaults = dict(lr=lr, weight_decay=weight_decay, betas=betas, eps=eps)\n",
        "        super(LAMB, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            # If a closure is provided, evaluate it\n",
        "            loss = closure()\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    # Skip parameters that have no gradient\n",
        "                    continue\n",
        "                grad = p.grad.data # Get the gradient of the parameter\n",
        "                # Retrieve or initialize the optimizer's internal state\n",
        "                state = self.state[p]\n",
        "                # Initialize state\n",
        "                if len(state) == 0:\n",
        "                    # Initialize the state (step counter, first moment, second moment)\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data) # First moment (mean of gradients)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data) # Second moment (mean of squared gradients)\n",
        "                exp_avg = state['exp_avg']\n",
        "                exp_avg_sq = state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']  # Coefficients for moment updates\n",
        "                state['step'] += 1 # Increment step count\n",
        "                step = state['step']\n",
        "                # Decay the first and second moment running average coefficients\n",
        "                # Update first and second moments\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1) # Exponential moving average of gradients\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2) # Exponential moving average of squared gradients\n",
        "                # Correct bias in the first and second moments\n",
        "                bias_correction1 = 1 - beta1 ** step\n",
        "                bias_correction2 = 1 - beta2 ** step\n",
        "                corrected_exp_avg = exp_avg / bias_correction1\n",
        "                corrected_exp_avg_sq = exp_avg_sq / bias_correction2\n",
        "                # Compute the denominator for scaling the update\n",
        "                denom = corrected_exp_avg_sq.sqrt().add_(group['eps']) # Stability epsilon\n",
        "                # Compute the step update\n",
        "                update = corrected_exp_avg / denom\n",
        "                # Apply weight decay directly to the parameters\n",
        "                if group['weight_decay'] != 0:\n",
        "                    update.add_(p.data, alpha=group['weight_decay'])\n",
        "                # Compute the trust ratio (norm of parameters vs norm of update)\n",
        "                param_norm = torch.norm(p.data)\n",
        "                update_norm = torch.norm(update)\n",
        "                trust_ratio = 1.0  # Default trust ratio\n",
        "                if param_norm > 0 and update_norm > 0:\n",
        "                    trust_ratio = param_norm / update_norm\n",
        "                # Update the parameters\n",
        "                p.data.add_(update, alpha=-group['lr'] * trust_ratio)\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Large-Batch Optimizers')\n",
        "parser.add_argument('--optimizer', type=str, default='SGDM', choices=['SGDM', 'AdamW', 'LARS', 'LAMB'],\n",
        "                    help='Choose optimizer')\n",
        "parser.add_argument('--batch-size', type=int, default=128, help='Batch size')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGDM and LARS')\n",
        "parser.add_argument('--weight-decay', type=float, default=1e-5, help='Weight decay')\n",
        "parser.add_argument('--b1', type=float, default=0.9, help='Beta1 for AdamW')\n",
        "parser.add_argument('--b2', type=float, default=0.999, help='Beta2 for AdamW')\n",
        "parser.add_argument('--trust-coefficient', type=float, default=0.001, help='Trust coefficient for LARS')\n",
        "parser.add_argument('--epochs', type=int, default=150, help='Number of epochs')\n",
        "parser.add_argument('--warmup-epochs', type=int, default=5, help='Number of epochs')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# Fixed parameter from paper [18] to calculate the square root LR scaling value\n",
        "# base_lr = 5 / (2**3 * 10**3)\n",
        "# reference_batch_size = 512\n",
        "#scaled_lr = base_lr * (args.batch_size / reference_batch_size) ** 0.5\n",
        "\n",
        "# Mapping optimizers\n",
        "if args.optimizer == 'SGDM':\n",
        "    base_lr = 1e-2\n",
        "    reference_batch_size = 64\n",
        "    scaled_lr = base_lr * (args.batch_size / reference_batch_size) ** 0.5\n",
        "    optimizer = optim.SGD(net.parameters(), lr=scaled_lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "elif args.optimizer == 'AdamW':\n",
        "    base_lr = 5e-4\n",
        "    reference_batch_size = 64\n",
        "    scaled_lr = base_lr * (args.batch_size / reference_batch_size) ** 0.5\n",
        "    optimizer = optim.AdamW(net.parameters(), lr=scaled_lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)\n",
        "elif args.optimizer == 'LARS':\n",
        "    base_lr = 1e-2\n",
        "    reference_batch_size = 64\n",
        "    scaled_lr = base_lr * (args.batch_size / reference_batch_size) ** 0.5\n",
        "    optimizer = LARS(net.parameters(), lr=scaled_lr, momentum=args.momentum, weight_decay=args.weight_decay, trust_coefficient=args.trust_coefficient)\n",
        "elif args.optimizer == 'LAMB':\n",
        "    base_lr = 5e-4\n",
        "    reference_batch_size = 64\n",
        "    scaled_lr = base_lr * (args.batch_size / reference_batch_size) ** 0.5\n",
        "    optimizer = LAMB(net.parameters(), lr=scaled_lr, weight_decay=args.weight_decay)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Scheduler initialization\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=args.epochs - args.warmup_epochs)\n",
        "\n",
        "# Training Function\n",
        "def train_model(optimizer, scheduler, model, criterion, trainloader , testloader, device, epochs, save_checkpoint_interval=10):\n",
        "    train_losses, test_losses = [], []\n",
        "    train_accuracies, test_accuracies = [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += predicted.eq(labels).sum().item()\n",
        "        train_loss = running_loss / len(trainloader)\n",
        "        train_accuracy = 100. * correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Test\n",
        "        correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in testloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += predicted.eq(labels).sum().item()\n",
        "        test_loss /= len(testloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracy = 100. * correct_test / total_test\n",
        "        test_accuracies.append(test_accuracy)\n",
        "\n",
        "        if epoch >= args.warmup_epochs:\n",
        "            scheduler.step()  # Trigger cosine decay\n",
        "        else:\n",
        "            warmup_factor = (epoch + 1) / args.warmup_epochs\n",
        "            lr = warmup_factor * scaled_lr\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "        # Checkpointing\n",
        "        if (epoch + 1) % save_checkpoint_interval == 0:\n",
        "            checkpoint_filename = f'checkpoint_epoch_{epoch + 1}.pth'\n",
        "            checkpoint = {\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'loss': loss.item(),\n",
        "            }\n",
        "            checkpoint_path = os.path.join('./', checkpoint_filename)\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            print(f'Checkpoint saved at epoch {epoch + 1}: {checkpoint_path}')\n",
        "\n",
        "            # Download the checkpoint\n",
        "            files.download(checkpoint_filename)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
        "\n",
        "# Train the model\n",
        "print(f\"--- Train with {args.optimizer} ---\")\n",
        "train_loss, test_loss, train_acc, test_acc = train_model(optimizer, scheduler, net, criterion, trainloader, testloader, device, args.epochs)\n",
        "\n",
        "# Save model\n",
        "torch.save(net.state_dict(), f'net_{args.optimizer}.pth')\n",
        "\n",
        "# Plot results\n",
        "# Plot Training Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_loss, label='Train Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Training Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(train_acc, label='Train Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_loss, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_acc, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hEcpkQqYJmP",
        "outputId": "c8eedbc5-9061-45df-d650-9e8a0ee8ae51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 38.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with SGDM ---\n",
            "Epoch 1/150, Train Acc: 5.64%, Test Acc: 11.74%\n",
            "Epoch 1/150, Train Loss: 4.2260, Test Loss: 3.8022\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer SGDM --batch-size 128 --weight-decay 4e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2t__56QmDKL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqIzBQKfZFgB",
        "outputId": "beff2b9d-73ac-4a8d-ed6e-51a1140977bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with AdamW ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer AdamW --batch-size 128 --weight-decay 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y8TL9oBZL18",
        "outputId": "54fc2f89-7e7d-461b-fce6-1f0da7310a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with LARS ---\n"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer LARS --batch-size 128 --weight-decay 4e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "K_XY2BkhZOkY",
        "outputId": "d4c6a687-f11c-439f-bbc4-b0d05ad58aca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 40.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Train with LAMB ---\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'val_accuracy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/content/large_batch_training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Train with {args.optimizer} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/large_batch_training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(optimizer, scheduler, model, criterion, trainloader, testloader, device, epochs, save_checkpoint_interval)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_accuracy' is not defined"
          ]
        }
      ],
      "source": [
        "%run large_batch_training.py --optimizer LAMB --batch-size 128 --weight-decay 1e-1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
