{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/Project-5-Distributed-Learning/blob/main/code/Personal_contribution/DynamicSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sSBZt98ePGy",
        "outputId": "c642e598-1e92-4926-ef47-6e897074b569"
      },
      "outputs": [],
      "source": [
        "%%writefile DynamicSGD.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "total_computation_time = 0\n",
        "total_communication_time = 0\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "iteration_per_epoch = 782 # number of local step performed in one epoch\n",
        "num_epochs = 150\n",
        "batch_size = 64\n",
        "base_learning_rate = 1e-3\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "\n",
        "# Hyperparameters for dynamic adjustment\n",
        "H_min, H_max = 4, 64  # Minimum and maximum allowed local steps\n",
        "alpha, epsilon = 1000, 1e-6  # Scaling factor and small constant\n",
        "\n",
        "def compute_mean_std(dataset):\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "# After creating the DataLoader for each worker, create an iterator\n",
        "shard_iterators = [iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True)) for k in range(K)]\n",
        "\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate: {learning_rate:.5f}\")\n",
        "\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for net in nets[1:]:  # Skip nets[0] because it's the reference model\n",
        "        for param_target, param_source in zip(net.parameters(), nets[0].parameters()):\n",
        "            param_target.data.copy_(param_source.data)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs) for optimizer in optimizers]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize storage for train losses and accuracies for each worker\n",
        "train_losses_per_worker = [[] for _ in range(K)]\n",
        "train_accuracies_per_worker = [[] for _ in range(K)]\n",
        "\n",
        "# Initialize test losses and accuracies for test set\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Compute gradient magnitude and variance\n",
        "def compute_gradient_stats(nets, optimizers):\n",
        "    # Get gradients from each worker\n",
        "    gradients = []\n",
        "    for net, optimizer in zip(nets, optimizers):\n",
        "        grad_vector = []\n",
        "        for param in net.parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_vector.append(param.grad.view(-1))\n",
        "        grad_vector = torch.cat(grad_vector)  # Flatten all gradients into a vector\n",
        "        gradients.append(grad_vector)\n",
        "\n",
        "    # Compute average gradient (g_t)\n",
        "    avg_gradient = torch.stack(gradients).mean(dim=0)\n",
        "\n",
        "    # Compute squared norms of individual gradients\n",
        "    squared_norms = [torch.norm(g).item() ** 2 for g in gradients]\n",
        "\n",
        "    # Compute squared norm of the average gradient\n",
        "    avg_squared_norm = torch.norm(avg_gradient).item() ** 2\n",
        "\n",
        "    # Compute gradient variance (sigma_t^2) using the provided formula\n",
        "    sigma_t_squared = (sum(squared_norms) / (K - 1)) - (K / (K - 1)) * avg_squared_norm\n",
        "\n",
        "    # Compute gradient magnitude (G_t) using the provided formula\n",
        "    G_t = avg_squared_norm - (1 / K) * sigma_t_squared\n",
        "\n",
        "    return G_t, sigma_t_squared\n",
        "\n",
        "\n",
        "# Update H dynamically based on gradient stats\n",
        "def update_H(G_t, sigma_t_squared, H_min, H_max, alpha, epsilon):\n",
        "    H_t = min(H_max, max(H_min, alpha * G_t / (sigma_t_squared + epsilon)))\n",
        "    return int(H_t)  # Ensure H_t is an integer\n",
        "\n",
        "# Training loop\n",
        "\n",
        "\n",
        "H_t = J  # Initialize H_t with the default J value\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_for_iteration = [0.0 for _ in range(K)]\n",
        "    correct_train_for_iteration = [0 for _ in range(K)]\n",
        "    total_train_for_iteration = [0 for _ in range(K)]\n",
        "\n",
        "    iteration_per_epoch = 782\n",
        "    iteration = 0 # Number of global iteration performed\n",
        "    while iteration_per_epoch > 0:\n",
        "        computation_start_time = time.time()\n",
        "        # Train each worker for H_t steps\n",
        "        for k in range(K):\n",
        "            nets[k].train()\n",
        "            correct_train, total_train, train_loss = 0, 0, 0.0\n",
        "            for _ in range(H_t):  # Perform H_t local steps\n",
        "                try:\n",
        "                    images, labels = next(shard_iterators[k])\n",
        "                except StopIteration:\n",
        "                    shard_iterators[k] = iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True))\n",
        "                    images, labels = next(shard_iterators[k])\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizers[k].zero_grad()\n",
        "                outputs = nets[k](images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizers[k].step()\n",
        "\n",
        "                # Calculate train loss and accuracy\n",
        "                train_loss += loss.item() * labels.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_loss_for_iteration[k] += train_loss\n",
        "            correct_train_for_iteration[k] += correct_train\n",
        "            total_train_for_iteration[k] += total_train\n",
        "\n",
        "        computation_end_time = time.time()\n",
        "        total_computation_time += (computation_end_time - computation_start_time)\n",
        "\n",
        "        communication_start_time = time.time()\n",
        "        # Synchronize models\n",
        "        with torch.no_grad():\n",
        "            global_parameters = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "            for net in nets:\n",
        "                for global_param, local_param in zip(global_parameters, net.parameters()):\n",
        "                    global_param += local_param\n",
        "            for global_param in global_parameters:\n",
        "                global_param /= K\n",
        "            for net in nets:\n",
        "                for local_param, global_param in zip(net.parameters(), global_parameters):\n",
        "                    local_param.data.copy_(global_param)\n",
        "\n",
        "        communication_end_time = time.time()\n",
        "        total_communication_time += (communication_end_time - communication_start_time)\n",
        "\n",
        "        # update tot_num_local_step and number of global iteration\n",
        "        iteration_per_epoch -= (H_t * K)\n",
        "        iteration += 1\n",
        "        # Compute gradient stats and update H_t\n",
        "        G_t, sigma_t_squared = compute_gradient_stats(nets, optimizers)\n",
        "        H_t = update_H(G_t, sigma_t_squared, H_min, H_max, alpha, epsilon)\n",
        "        print(f\"Epoch {epoch+1}, Iteration {iteration}: H_t = {H_t}, G_t = {G_t:.4f}, σ_t^2 = {sigma_t_squared:.4f}\")\n",
        "\n",
        "    # Store training metrics for all workers after each epoch\n",
        "    for k in range(K):\n",
        "        # Store train loss and accuracy for this worker\n",
        "        train_loss_for_iteration[k] /= total_train_for_iteration[k]\n",
        "        train_losses_per_worker[k].append(train_loss_for_iteration[k])\n",
        "        tot_accuracy = 100. * correct_train_for_iteration[k] / total_train_for_iteration[k]\n",
        "        train_accuracies_per_worker[k].append(tot_accuracy)\n",
        "        print(f\"    Worker {k}: Train Loss: {train_losses_per_worker[k][-1]:.4f}, Train Accuracy: {train_accuracies_per_worker[k][-1]:.2f}%\")\n",
        "\n",
        "    # Update learning rate\n",
        "    for scheduler in schedulers:\n",
        "        scheduler.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    global_model = nets[0]\n",
        "    global_model.eval()\n",
        "    correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = 100. * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    print(f\"Epoch {epoch + 1}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    print(f\"Total Computation Time: {total_computation_time:.2f} seconds\")\n",
        "    print(f\"Total Communication Time: {total_communication_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# Plot Train Loss for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_losses_per_worker[k], label=f'Worker {k} Train Loss')\n",
        "plt.title('Train Loss per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracy for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_accuracies_per_worker[k], label=f'Worker {k} Train Accuracy')\n",
        "plt.title('Train Accuracy per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MMfqfbnNFSn3",
        "outputId": "d07027af-bcf8-4ec6-9431-880bdd1596cf"
      },
      "outputs": [],
      "source": [
        "%run DynamicSGD.py --k 4 --j 32"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
