{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucavgn/AML_Project5/blob/main/Distributed_LocalSGD_SlowMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqBQ7RmFZ_X",
        "outputId": "fc1ea2b1-836b-4490-8777-d96a950a134f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing localSGD.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile localSGD_SlowMo.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from torch.utils.data import random_split, Subset, DataLoader\n",
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "total_computation_time = 0\n",
        "total_communication_time = 0\n",
        "\n",
        "# Define hyperparameters\n",
        "# K_values = [2, 4, 8]\n",
        "# J_values = [4, 8, 16, 32, 64]\n",
        "# Alpha slow learning rate \n",
        "# Beta slow momentum\n",
        "global_iteration_per_epoch = 782 # to be divided by K*J\n",
        "num_epochs = 150\n",
        "batch_size = 64\n",
        "base_learning_rate = 1e-3\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-4\n",
        "#warmup_epochs = 5\n",
        "\n",
        "alpha = 1.0  # Slow learning rate scaling factor\n",
        "beta = 0  # Slow momentum factor\n",
        "# Compute the mean and std of CIFAR-100 dataset\n",
        "def compute_mean_std(dataset):\n",
        "\n",
        "    # Extract images and labels\n",
        "    data_r = np.stack([np.array(dataset[i][0])[:, :, 0] for i in range(len(dataset))])\n",
        "    data_g = np.stack([np.array(dataset[i][0])[:, :, 1] for i in range(len(dataset))])\n",
        "    data_b = np.stack([np.array(dataset[i][0])[:, :, 2] for i in range(len(dataset))])\n",
        "\n",
        "    # Compute mean and std\n",
        "    mean = np.mean(data_r), np.mean(data_g), np.mean(data_b)\n",
        "    std = np.std(data_r), np.std(data_g), np.std(data_b)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "# Define LeNet-5 architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.relu4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Command-line arguments\n",
        "parser = argparse.ArgumentParser(description='Train with Local SGD')\n",
        "parser.add_argument('--k', type=int, default=2, help='choose a K value for local SGD: [2, 4, 8]')\n",
        "parser.add_argument('--j', type=int, default=4, help='choose a J value for local SGD: [4, 8, 16, 32, 64]')\n",
        "args = parser.parse_args()\n",
        "K = args.k\n",
        "J = args.j\n",
        "print(f\"Training with K={K}, J={J}\")\n",
        "\n",
        "global_iteration_per_epoch = math.ceil(global_iteration_per_epoch / (K * J)) * (K * J)\n",
        "global_iteration_per_epoch = global_iteration_per_epoch // (K * J)\n",
        "print(f\"Global iteration per epoch: {global_iteration_per_epoch}\")\n",
        "\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(42) # Set the seed for reproducibility\n",
        "torch.cuda.manual_seed_all(42) # Set the seed for reproducibility on GPU\n",
        "\n",
        "# use the same mean and std to add consistency to all datasets\n",
        "data = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mean, std = compute_mean_std(data)\n",
        "\n",
        "# Load and split CIFAR-100 dataset\n",
        "train_transform = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  \n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Shard the training set\n",
        "shard_size = len(trainset) // K\n",
        "shards = [Subset(trainset, range(i * shard_size, (i + 1) * shard_size)) for i in range(K)]\n",
        "# After creating the DataLoader for each worker, create an iterator\n",
        "shard_iterators = [iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True)) for k in range(K)]\n",
        "\n",
        "\n",
        "# Test set\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Scale the learning rate by k\n",
        "learning_rate = base_learning_rate * K\n",
        "print(f\"Base learning rate for each k: {learning_rate:.5f}\")\n",
        "# Initialize local models, optimizers and scheduler\n",
        "nets = [LeNet5().to(device) for _ in range(K)]\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for net in nets[1:]:  # Skip nets[0] because it's the reference model\n",
        "        for param_target, param_source in zip(net.parameters(), nets[0].parameters()):\n",
        "            param_target.data.copy_(param_source.data)\n",
        "\n",
        "optimizers = [optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay) for net in nets]\n",
        "schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=num_epochs) for optimizer in optimizers]\n",
        "\n",
        "# Momentum buffer\n",
        "u_buffer = [torch.zeros_like(param) for param in nets[0].parameters()]  \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize storage for train losses and accuracies for each worker\n",
        "train_losses_per_worker = [[] for _ in range(K)]\n",
        "train_accuracies_per_worker = [[] for _ in range(K)]\n",
        "\n",
        "# Initialize test losses and accuracies for test set\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_for_iteration = [0.0 for _ in range(K)]\n",
        "    correct_train_for_iteration = [0 for _ in range(K)]\n",
        "    total_train_for_iteration = [0 for _ in range(K)]\n",
        "    for iteration in range(global_iteration_per_epoch): #outer loop\n",
        "        computation_start_time = time.time()\n",
        "        #x(t,0)\n",
        "        iter_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "        for param, g_param in zip(iter_params, nets[0].parameters()):\n",
        "                    param.data.copy_(g_param)\n",
        "        # Sequentially train each worker to simulate a distributed environment\n",
        "        for k in range(K):\n",
        "\n",
        "            nets[k].train()\n",
        "            correct_train, total_train, train_loss = 0, 0, 0.0\n",
        "\n",
        "            # Perform J local step before synchronization\n",
        "            for j in range(J): #inner loop\n",
        "                      try:\n",
        "                          # Get a batch from the current worker's shard using next()\n",
        "                          images, labels = next(shard_iterators[k])\n",
        "                          images, labels = images.to(device), labels.to(device)  # Move to device\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "                      except StopIteration:\n",
        "                          # If the iterator has exhausted the data, restart the iterator\n",
        "                          shard_iterators[k] = iter(DataLoader(shards[k], batch_size=batch_size, shuffle=True))\n",
        "                          images, labels = next(shard_iterators[k]) # Resume from the next batch\n",
        "                          images, labels = images.to(device), labels.to(device)\n",
        "                          # Perform optimization step\n",
        "                          optimizers[k].zero_grad()\n",
        "                          outputs = nets[k](images)\n",
        "                          loss = criterion(outputs, labels)\n",
        "                          loss.backward()\n",
        "                          optimizers[k].step()\n",
        "                          # Calculate train loss and accuracy\n",
        "                          train_loss += loss.item() * labels.size(0)\n",
        "                          _, predicted = outputs.max(1)\n",
        "                          total_train += labels.size(0)\n",
        "                          correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "            train_loss_for_iteration[k] += train_loss\n",
        "            correct_train_for_iteration[k] += correct_train\n",
        "            total_train_for_iteration[k] += total_train\n",
        "\n",
        "        computation_end_time = time.time()\n",
        "        total_computation_time += (computation_end_time - computation_start_time)\n",
        "\n",
        "        communication_start_time = time.time()\n",
        "          # Synchronization (SLOWMO Algothm 1 - paper 21)\n",
        "        with torch.no_grad():\n",
        "            global_params = [torch.zeros_like(param) for param in nets[0].parameters()]\n",
        "            #x(t, tau)\n",
        "            for net in nets:\n",
        "                for global_param, local_param in zip(global_params, net.parameters()):\n",
        "                    global_param += local_param\n",
        "            for global_param in global_params:\n",
        "                global_param /= K\n",
        "\n",
        "            for param, g_param, u_param in zip(iter_params, global_params, u_buffer):\n",
        "                    lr_sched = optimizers[0].param_groups[0]['lr'] #optimizer lr\n",
        "                    u_param.mul_(beta).add_((param - g_param) / lr_sched)\n",
        "                    g_param.copy_(param - alpha * lr_sched * u_param)\n",
        "\n",
        "            for net in nets:\n",
        "                for param, g_param in zip(net.parameters(), global_params):\n",
        "                    param.data.copy_(g_param)\n",
        "\n",
        "        communication_end_time = time.time()\n",
        "        total_communication_time += (communication_end_time - communication_start_time)\n",
        "\n",
        "    # Update lr for each scheduler\n",
        "    for scheduler in schedulers:\n",
        "            scheduler.step()  # Trigger cosine decay\n",
        "\n",
        "    # Final Evaluation on Test Set\n",
        "    global_model = nets[0] # all models are synchronized\n",
        "    global_model.eval()\n",
        "    # Test\n",
        "    correct_test, total_test, test_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += predicted.eq(labels).sum().item()\n",
        "    test_loss /= len(testloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = 100. * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    for k in range(K):\n",
        "        # Store train loss and accuracy for this worker\n",
        "        train_loss_for_iteration[k] /= total_train_for_iteration[k]\n",
        "        train_losses_per_worker[k].append(train_loss_for_iteration[k])\n",
        "        tot_accuracy = 100. * correct_train_for_iteration[k] / total_train_for_iteration[k]\n",
        "        train_accuracies_per_worker[k].append(tot_accuracy)\n",
        "        print(f\"    Worker {k}: Train Loss: {train_losses_per_worker[k][-1]:.4f}, Train Accuracy: {train_accuracies_per_worker[k][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"Total Computation Time: {total_computation_time:.2f} seconds\")\n",
        "    print(f\"Total Communication Time: {total_communication_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# Plot results\n",
        "# Plot Train Loss for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_losses_per_worker[k], label=f'Worker {k} Train Loss')\n",
        "plt.title('Train Loss per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_loss_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Train Accuracy for Each Worker\n",
        "plt.figure(figsize=(8, 6))\n",
        "for k in range(K):\n",
        "    plt.plot(train_accuracies_per_worker[k], label=f'Worker {k} Train Accuracy')\n",
        "plt.title('Train Accuracy per Worker')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('train_accuracy_per_worker.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(test_accuracies, label='Test Accuracy')\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('test_accuracy.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW9c_PRbwGQw",
        "outputId": "79f85ee8-22d2-4cfc-d760-82f67756817d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with K=1, J=1\n",
            "Global iteration per epoch: 782\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 72.0MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Base learning rate for each k: 0.00100\n",
            "Epoch 1/150: Test Loss: 4.5696, Test Accuracy: 2.06%\n",
            "    Worker 0: Train Loss: 4.5956, Train Accuracy: 1.37%\n",
            "Epoch 2/150: Test Loss: 4.1204, Test Accuracy: 7.11%\n",
            "    Worker 0: Train Loss: 4.4005, Train Accuracy: 4.01%\n",
            "Epoch 3/150: Test Loss: 3.8997, Test Accuracy: 10.08%\n",
            "    Worker 0: Train Loss: 4.0304, Train Accuracy: 8.47%\n",
            "Epoch 4/150: Test Loss: 3.7648, Test Accuracy: 12.54%\n",
            "    Worker 0: Train Loss: 3.8659, Train Accuracy: 11.08%\n"
          ]
        }
      ],
      "source": [
        "%run localSGD_SlowMo.py --k 1 --j 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
